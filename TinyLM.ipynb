{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOZne/dsJ6iLr6dhckF4nCp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amantayal44/TinyLM/blob/main/TinyLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language Model From Scratch\n",
        "\n",
        "In this notebook, we are going to implement different LM model with small configurations such as GPT-2, Llama2, etc.\n",
        "\n",
        "Compare impact of different architectural changes such as shared embedding, RoPe Positional Encoding, Normalization techniques, etc.\n",
        "\n",
        "To make it easier to train on colab we will be using tiny shakespeare dataset.\n",
        "\n",
        "Initially, for all model we will try to keep almost similar parameters count and then we will try to scale best model also try some different datasets"
      ],
      "metadata": {
        "id": "03MsEekai7sw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8jJbnqkifSq"
      },
      "outputs": [],
      "source": []
    }
  ]
}